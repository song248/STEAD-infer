import os
import math
import torch
import numpy as np
from tqdm import tqdm
from torch import nn
from pytorchvideo.data.encoded_video import EncodedVideo

# =======================
# GPU ê³ ì •: 1ë²ˆ GPUë§Œ ì‚¬ìš©
# =======================
# ì£¼ì˜: ì´ ì„¤ì •ì€ torchì˜ CUDA ì»¨í…ìŠ¤íŠ¸ê°€ ìƒì„±ë˜ê¸° ì „ì— ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

# ===== CONFIG =====
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# ì…ë ¥/ì¶œë ¥ í´ë” ì„¤ì •
input_root_folder = "my_video_ver2"          # my_video/{normal, violence}
output_root_folder = "my_video_ver2_npy"     # my_video_npy/{normal, assault}

# í´ë˜ìŠ¤ ë§¤í•‘: ì…ë ¥ í´ë”ëª… -> ì¶œë ¥ í´ë”ëª…
class_map = {
    "normal": "normal",
    "violence": "assault",
}

os.makedirs(output_root_folder, exist_ok=True)
for _in, _out in class_map.items():
    os.makedirs(os.path.join(output_root_folder, _out), exist_ok=True)

valid_exts = [".mp4", ".avi", ".mov", ".mkv"]

# ===== X3D ì„¤ì • =====
model_name = 'x3d_l'
transform_params = {
    "num_frames": 15,
    "sampling_rate": 4,
    "side_size": 320,
    "crop_size": 320,
    "frames_per_second": 30,
}
mean = [0.45, 0.45, 0.45]
std = [0.225, 0.225, 0.225]

# ===== Normalize í•¨ìˆ˜ =====
def normalize_video_tensor(video_tensor, mean, std):
    mean = torch.tensor(mean, device=video_tensor.device).view(1, -1, 1, 1)
    std = torch.tensor(std, device=video_tensor.device).view(1, -1, 1, 1)
    return (video_tensor - mean) / std

# ===== ê³ ì • ê°„ê²© ìƒ˜í”Œë§ =====
def fixed_interval_sample(frames, interval, num_samples):
    total_needed = interval * num_samples
    if frames.shape[0] < total_needed:
        pad = frames[-1:].repeat(total_needed - frames.shape[0], 1, 1, 1)
        frames = torch.cat([frames, pad], dim=0)
    indices = torch.arange(0, total_needed, step=interval)
    return frames[indices]

# ===== ì „ì²˜ë¦¬ í•¨ìˆ˜ =====
def preprocess_frames(frames):
    # ì…ë ¥ frames: (T,H,W,C) ë˜ëŠ” (3,T,H,W)ì—ì„œ ë³€í™˜ë¨
    frames = frames.permute(0, 3, 1, 2) / 255.0  # (T,H,W,C)â†’(T,C,H,W)
    frames = normalize_video_tensor(frames, mean, std)
    frames = torch.nn.functional.interpolate(
        frames, size=(transform_params["side_size"], transform_params["side_size"]),
        mode="bilinear", align_corners=False
    )
    crop = transform_params["crop_size"]
    center = transform_params["side_size"] // 2
    frames = frames[:, :, center - crop//2:center + crop//2,
                          center - crop//2:center + crop//2]
    frames = frames.permute(1, 0, 2, 3)  # (T,C,H,W)â†’(C,T,H,W)
    return frames

# ===== X3D ëª¨ë¸ ë¡œë”© =====
# ë¶„ë¥˜ í—¤ë“œë¥¼ ì œê±°í•˜ì—¬ ì¤‘ê°„ íŠ¹ì§•ë§Œ ì¶”ì¶œ
x3d = torch.hub.load('facebookresearch/pytorchvideo', model_name, pretrained=True)
x3d = nn.Sequential(*list(x3d.blocks[:5]))  # classification head ì œê±°
x3d = x3d.to(device).eval()

# ===== í´ë¦½ë³„ íŠ¹ì§• ì¶”ì¶œ ë° ì €ì¥ =====
def extract_and_save_features(video_path, save_path):
    video = EncodedVideo.from_path(video_path)
    fps = transform_params["frames_per_second"]
    duration = float(video.duration)
    window_sec = (transform_params["num_frames"] * transform_params["sampling_rate"]) / fps
    stride_sec = 1.0

    features = []

    total_steps = max(1, math.ceil((duration - window_sec) / stride_sec) + 1)
    for step in tqdm(range(total_steps), desc="Extracting features", leave=False, ncols=80):
        start = step * stride_sec
        end = start + window_sec

        try:
            clip = video.get_clip(start_sec=start, end_sec=end)
        except Exception:
            break

        frames = clip["video"]
        # pytorchvideoëŠ” (C,T,H,W) í˜•ì‹ì¼ ìˆ˜ ìˆìŒ
        if frames.shape[0] == 3:
            frames = frames.permute(1, 2, 3, 0)  # (C,T,H,W)â†’(T,H,W,C)
        elif frames.shape[-1] != 3:
            raise ValueError(f"Unsupported frame shape: {frames.shape}")

        frames = fixed_interval_sample(frames, transform_params["sampling_rate"], transform_params["num_frames"])
        frames = preprocess_frames(frames).unsqueeze(0).to(device)

        with torch.no_grad():
            feature = x3d(frames).squeeze(0).cpu().numpy()  # (C,T,H,W) â†’ np.array
            features.append(feature)

    if len(features) == 0:
        print(f"âš ï¸  No features extracted for: {video_path} (skipped)")
        return False

    features = np.stack(features)  # (N_clips, C, T, H, W)
    np.save(save_path, features)
    print(f"âœ… Saved: {save_path}")
    return True

# ===== MAIN =====
if __name__ == "__main__":
    total_videos = 0
    processed = 0

    for in_cls, out_cls in class_map.items():
        in_dir = os.path.join(input_root_folder, in_cls)
        out_dir = os.path.join(output_root_folder, out_cls)
        os.makedirs(out_dir, exist_ok=True)

        if not os.path.isdir(in_dir):
            print(f"âš ï¸  Input class folder not found: {in_dir} (skipping)")
            continue

        video_files = [
            f for f in sorted(os.listdir(in_dir))
            if os.path.splitext(f)[-1].lower() in valid_exts
        ]

        print(f"\n=== Class: {in_cls} â†’ {out_cls} | {len(video_files)} files ===")
        total_videos += len(video_files)

        for idx, fname in enumerate(video_files, start=1):
            print(f"[{idx}/{len(video_files)}] Processing {fname}")
            video_path = os.path.join(in_dir, fname)
            video_name = os.path.splitext(fname)[0]
            save_path = os.path.join(out_dir, f"{video_name}.npy")

            try:
                ok = extract_and_save_features(video_path, save_path)
                if ok:
                    processed += 1
            except Exception as e:
                print(f"ğŸš¨ Error on {fname}: {e}")
                continue

    print(f"\nâœ… Done. {processed}/{total_videos} videos processed.")
