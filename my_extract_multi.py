import os
import torch
import numpy as np
from tqdm import tqdm
from multiprocessing import Process
from pytorchvideo.data.encoded_video import EncodedVideo
from torch import nn
import torch.nn.functional as F
import av
from contextlib import nullcontext

# ====== CONFIG ======
VIDEO_ROOT = 'my_video'          # ì…ë ¥ ë£¨íŠ¸: my_video/{normal, violence}
OUTPUT_ROOT = 'my_video_npy'     # ì¶œë ¥ ë£¨íŠ¸: my_video_npy/{normal, assault}
CLASS_MAP = {                    # ì…ë ¥ í´ë”ëª… -> ì¶œë ¥ í´ë”ëª… ë§¤í•‘
    'normal': 'normal',
    'violence': 'assault',
}
MODEL_NAME = 'x3d_l'
GPUS = [0, 1]

# (ê¸°ì¡´ 16 â†’ 8ë¡œ í•˜í–¥: í•„ìš” ì‹œ 4ê¹Œì§€ ë‚®ì¶”ì„¸ìš”)  :contentReference[oaicite:2]{index=2}
BATCH_SIZE = 8

# X3D-L ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° (ì›ë³¸ ì˜ë„ ìœ ì§€)  :contentReference[oaicite:3]{index=3}
SIDE_SIZE = 320
CROP_SIZE = 320
NUM_FRAMES = 4
SAMPLING_RATE = 5
MEAN = torch.tensor([0.45, 0.45, 0.45]).view(1, 3, 1, 1)   # (1,C,1,1)
STD  = torch.tensor([0.225, 0.225, 0.225]).view(1, 3, 1, 1)

# AMP ì‚¬ìš© ì—¬ë¶€ (True ê¶Œì¥)
USE_AMP = True


# ====== Helpers ======
class Permute(nn.Module):
    def __init__(self, dims):
        super().__init__()
        self.dims = dims
    def forward(self, x):
        return x.permute(*self.dims)


def get_fps(video_path, default=30.0):
    try:
        container = av.open(video_path)
        for stream in container.streams:
            if stream.type == 'video' and stream.average_rate:
                return float(stream.average_rate)
    except:
        pass
    return default


@torch.no_grad()
def temporal_subsample(x, num_samples):
    """
    x: (T, C, H, W) or (C, T, H, W) â†’ ë°˜í™˜ì€ (T, C, H, W)
    ê· ì¼ ê°„ê²©ìœ¼ë¡œ num_samples ê°œ í”„ë ˆì„ ì„ íƒ
    """
    if x.dim() != 4:
        raise ValueError("expected 4D video tensor")
    # to (T, C, H, W)
    if x.shape[0] in (1, 3):   # (C, T, H, W)
        x = x.permute(1, 0, 2, 3)
    T = x.shape[0]
    if T == num_samples:
        return x
    idx = torch.linspace(0, max(T - 1, 0), steps=num_samples).round().long()
    idx = torch.clamp(idx, 0, T - 1)
    return x.index_select(0, idx)


@torch.no_grad()
def short_side_resize(x, size):
    """
    x: (T, C, H, W), ì§§ì€ ë³€ì„ sizeë¡œ ìœ ì§€í•˜ë©° ì¢…íš¡ë¹„ ë³´ì¡´ ë¦¬ì‚¬ì´ì¦ˆ
    """
    T, C, H, W = x.shape
    if H <= W:
        new_h, new_w = size, int(round(W * size / H))
    else:
        new_h, new_w = int(round(H * size / W)), size
    # (T,C,H,W) â†’ (T*C,1,H,W)ë¡œ í¼ì³ì„œ í•œ ë²ˆì— interpolate
    x = x.reshape(T * C, 1, H, W)
    x = F.interpolate(x, size=(new_h, new_w), mode='bilinear', align_corners=False)
    x = x.reshape(T, C, new_h, new_w)
    return x


@torch.no_grad()
def center_crop(x, crop_size):
    """
    x: (T, C, H, W) â†’ ì„¼í„° í¬ë¡­ (crop_size, crop_size)
    """
    T, C, H, W = x.shape
    if H < crop_size or W < crop_size:
        side = max(crop_size, min(H, W))
        x = short_side_resize(x, side)
        T, C, H, W = x.shape
    top = (H - crop_size) // 2
    left = (W - crop_size) // 2
    return x[:, :, top:top + crop_size, left:left + crop_size]


@torch.no_grad()
def normalize_01(x):
    # ì…ë ¥: (T, C, H, W), uint8 ë˜ëŠ” float â†’ 0~1 float
    if x.dtype != torch.float32:
        x = x.float()
    return x / 255.0


@torch.no_grad()
def standardize(x):
    # ì±„ë„ ì •ê·œí™”: (x - mean)/std, x: (T,C,H,W)
    return (x - MEAN.to(x.device)) / STD.to(x.device)


@torch.no_grad()
def preprocess_clip(video_dict):
    """
    EncodedVideo.get_clip(...)ì˜ ë°˜í™˜ dict ì´ìš©.
    ë°˜í™˜: (C, T, H, W) í…ì„œ (X3D ì…ë ¥ í˜•ìƒ). **CPU í…ì„œë¡œ ìœ ì§€!**
    (ì›ë³¸ì€ ì—¬ê¸°ì„œ .to(device)ë¡œ GPUì— ì˜¬ë ¤ ëˆ„ì í•˜ë˜ ê²ƒì´ OOM ì›ì¸)  :contentReference[oaicite:4]{index=4}
    """
    if "video" not in video_dict or video_dict["video"] is None:
        return None
    v = video_dict["video"]  # pytorchvideoëŠ” (C,T,H,W) ë˜ëŠ” (T,H,W,C)ì¼ ìˆ˜ ìˆìŒ
    if v.dim() != 4:
        return None

    # to (T, C, H, W)
    if v.shape[0] in (1, 3):   # (C, T, H, W)
        v = v.permute(1, 0, 2, 3)
    elif v.shape[-1] in (1, 3):  # (T, H, W, C)
        v = v.permute(0, 3, 1, 2)
    else:
        return None

    # 1) ì‹œê°„ ê· ì¼ ìƒ˜í”Œë§
    v = temporal_subsample(v, NUM_FRAMES)          # (T,C,H,W)
    # 2) 0~1 ìŠ¤ì¼€ì¼
    v = normalize_01(v)                            # (T,C,H,W)
    # 3) ë¦¬ì‚¬ì´ì¦ˆ/í¬ë¡­/ì •ê·œí™”
    v = short_side_resize(v, SIDE_SIZE)            # (T,C,h,w)
    v = center_crop(v, CROP_SIZE)                  # (T,C,CROP,CROP)
    v = standardize(v)                             # (T,C,*,*)
    # 4) (C,T,H,W)ë¡œ ë³€í™˜
    v = v.permute(1, 0, 2, 3).contiguous()         # (C,T,H,W)
    return v  # <-- CPU í…ì„œ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì—¬ê¸°ì„œ GPUë¡œ ì˜¬ë¦¬ì§€ ì•ŠìŒ)


def init_model(gpu_id):
    torch.cuda.set_device(gpu_id)
    model = torch.hub.load('facebookresearch/pytorchvideo', MODEL_NAME, pretrained=True)
    model = model.to(f'cuda:{gpu_id}').eval()
    # ë¶„ë¥˜ í—¤ë“œ ì œê±° â†’ í”¼ì²˜ ì¶”ì¶œìš© (ì›ë³¸ ë™ì¼)  :contentReference[oaicite:5]{index=5}
    del model.blocks[-1]
    return model


def list_videos_recursive(root, exts={'.mp4', '.avi', '.mov', '.mkv'}):
    for dirpath, _, filenames in os.walk(root):
        for fn in filenames:
            if os.path.splitext(fn)[1].lower() in exts:
                yield os.path.join(dirpath, fn)


def build_jobs():
    jobs = []
    for in_cls, out_cls in CLASS_MAP.items():
        in_dir = os.path.join(VIDEO_ROOT, in_cls)
        for vpath in list_videos_recursive(in_dir):
            rel = os.path.relpath(vpath, in_dir)  # í•˜ìœ„êµ¬ì¡° ë³´ì¡´
            spath = os.path.join(OUTPUT_ROOT, out_cls, os.path.splitext(rel)[0] + '.npy')
            jobs.append((vpath, spath))
    return jobs


def process_video_batch(video_paths, gpu_id):
    device = f'cuda:{gpu_id}'
    torch.cuda.set_device(gpu_id)
    model = init_model(gpu_id)
    amp_ctx = torch.cuda.amp.autocast if USE_AMP else nullcontext

    print(f"[GPU {gpu_id}] Start {len(video_paths)} videos")

    for video_path, save_path in video_paths:
        try:
            fps = get_fps(video_path)
            clip_duration = (NUM_FRAMES * SAMPLING_RATE) / max(fps, 1e-6)

            video = EncodedVideo.from_path(video_path)
            if video.duration is None or video.duration < clip_duration:
                print(f"[GPU {gpu_id}] [SKIP] {video_path}: too short")
                continue

            total_clips = int(video.duration // clip_duration)

            feats = []
            # === ìŠ¤íŠ¸ë¦¬ë° ë°°ì¹˜ ì²˜ë¦¬: í´ë¦½ì„ ìŒ“ì•„ë‘ì§€ ì•Šê³  ì¦‰ì‹œ ë°°ì¹˜ êµ¬ì„± ===
            for i in range(0, total_clips, BATCH_SIZE):
                batch_cpu = []
                for j in range(i, min(i + BATCH_SIZE, total_clips)):
                    clip = video.get_clip(
                        start_sec=j * clip_duration,
                        end_sec=(j + 1) * clip_duration
                    )
                    v = preprocess_clip(clip)  # CPU í…ì„œ
                    if v is not None:
                        batch_cpu.append(v)

                if not batch_cpu:
                    continue

                # CPUì—ì„œ ìŠ¤íƒ â†’ pinned memory â†’ GPU ì „ì†¡
                batch = torch.stack(batch_cpu, dim=0)  # CPU (B,C,T,H,W)
                try:
                    batch = batch.pin_memory()
                except Exception:
                    pass
                batch = batch.to(device, non_blocking=True)

                with torch.inference_mode(), amp_ctx():
                    pred = model(batch)   # í˜•íƒœ: (B, ...)
                feats.append(pred.detach().cpu().numpy())

                # ì¦‰ì‹œ ì •ë¦¬
                del batch, batch_cpu, pred
                torch.cuda.empty_cache()

            if not feats:
                print(f"[GPU {gpu_id}] [SKIP] {video_path}: no valid clips")
                # ë¹„ë””ì˜¤ ê°ì²´ ì •ë¦¬
                try:
                    del video
                except Exception:
                    pass
                torch.cuda.empty_cache()
                continue

            feats = np.concatenate(feats, axis=0)  # (N_clips, ...)
            reduced = np.max(feats, axis=0)        # í´ë¦½ ì¶• max-pooling

            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            np.save(save_path, reduced)
            print(f"[GPU {gpu_id}] âœ… Saved: {save_path}")

            # íŒŒì¼ ë‹¨ìœ„ ìºì‹œ ì •ë¦¬
            del video, feats, reduced
            torch.cuda.empty_cache()

        except Exception as e:
            print(f"[GPU {gpu_id}] [ERROR] {video_path}: {e}")
            torch.cuda.empty_cache()


def main():
    jobs = build_jobs()
    print(f"ğŸš€ Total {len(jobs)} videos across {len(GPUS)} GPUs")
    # ë¼ìš´ë“œë¡œë¹ˆ ë¶„ë°° (ì› ì½”ë“œì™€ ë™ì¼ ì•„ì´ë””ì–´)  :contentReference[oaicite:6]{index=6}
    splits = [[] for _ in range(len(GPUS))]
    for i, job in enumerate(jobs):
        splits[i % len(GPUS)].append(job)

    procs = []
    for gpu_id, sub in zip(GPUS, splits):
        p = Process(target=process_video_batch, args=(sub, gpu_id))
        p.start()
        procs.append(p)
    for p in procs:
        p.join()


if __name__ == "__main__":
    main()
